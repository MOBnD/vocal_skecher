import numpy as np
import whisper
import re
import srt
import cv2
import os
import random
from datetime import timedelta
from pydub import AudioSegment
from moviepy.editor import (
    VideoClip, AudioFileClip, CompositeAudioClip, VideoFileClip,
    TextClip, CompositeVideoClip, vfx, ImageClip
)
import scipy.io.wavfile as wavfile
import spacy
import pandas as pd
import librosa

#æ„Ÿæƒ…è¾æ›¸ã®æº–å‚™

print("[INFO] ğŸ’¡ æ„Ÿæƒ…è¾æ›¸ã‚’æ§‹ç¯‰ä¸­...")
#GiNZAæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
nlp = spacy.load("ja_ginza")

#SNOWD18èª­ã¿è¾¼ã¿
snow_d18_path = 'D18-2018.7.24.xlsx'
snow_d18_df = pd.read_excel(snow_d18_path, sheet_name="ä½œæ¥­è€…A")

snow_d18_mapping = {
    "å¥½": "å–œ", "è¦ª": "å–œ",
    "å®‰": "æ¥½", "å°Š": "æ¥½",
    "æ€’": "æ€’", "å«Œ": "æ€’",
    "æ‚²": "å“€", "ä¸": "å“€"
}

snow_d18_dict = {}
for index, row in snow_d18_df.iterrows():
    word = row['Word']
    emotions = list(str(row['Emotion']))
    for emo in emotions:
        mapped = snow_d18_mapping.get(emo)
        if mapped:
            snow_d18_dict[word] = mapped
            break

'''
# æ—¥æœ¬èªè©•ä¾¡æ¥µæ€§è¾æ›¸ï¼ˆPN Dictionaryï¼‰
pn_dict_path = 'pn.csv.m3.120408.trim'
pn_df = pd.read_csv(pn_dict_path, sep='\t', header=None, names=['word', 'polarity', 'annotation'])

pn_dict = {}
for index, row in pn_df.iterrows():
    if row['polarity'] == 'p':
        pn_dict[row['word']] = 'å–œ'
    elif row['polarity'] == 'n':
        pn_dict[row['word']] = 'å“€'
'''

# çµ±åˆæ„Ÿæƒ…è¾æ›¸
emotion_dict = {**snow_d18_dict}
#emotion_dict = {**snow_d18_dict, **pn_dict}

def get_emotion_from_dict(word):
    return emotion_dict.get(word, "ãªã—")

def analyze_sentiment_per_word(text):
    doc = nlp(text)
    word_emotions = []
    for token in doc:
        base_word = token.lemma_
        emotion = get_emotion_from_dict(base_word)
        word_emotions.append((token.text, emotion))
    return word_emotions

def get_dominant_emotion(text):
    emotions = [emo for _, emo in analyze_sentiment_per_word(text) if emo != "ãªã—"]
    if emotions:
        return max(set(emotions), key=emotions.count)
    return "ãªã—"


#Whisperã§ã®ä½¿ã£ãŸéŸ³å£°èªè­˜&å­—å¹•ä½œæˆ

def generate_subtitles(audio_path, output_path, offset_seconds=0, model_size="large"):
    print("ğŸ“œ Whisperãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    model = whisper.load_model(model_size)
    print(f"ğŸ¤ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« '{audio_path}' ã‚’æ–‡å­—èµ·ã“ã—ä¸­...")
    result = model.transcribe(audio_path)

    print(f"ğŸ“‚ å­—å¹•ã‚’ '{output_path}' ã«ä¿å­˜ä¸­...")
    with open(output_path, "w", encoding="utf-8") as f:
        for i, segment in enumerate(result["segments"], start=1):
            start = timedelta(seconds=int(segment["start"]) + offset_seconds)
            end = timedelta(seconds=int(segment["end"]) + offset_seconds)
            start_str = str(start)
            end_str = str(end)
            text = segment["text"].strip()
            start_srt = re.sub(r"\.(\d{3})\d{3}", r",\1", start_str)
            end_srt = re.sub(r"\.(\d{3})\d{3}", r",\1", end_str)
            f.write(f"{i}\n{start_srt} --> {end_srt}\n{text}\n\n")
    print("âœ… å­—å¹•ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")

#å­—å¹•ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ„Ÿæƒ…ä»˜ãï¼‰
    
def apply_advanced_animation(txt_clip, video_width, video_height, bpm, emotion="ãªã—"):
    txt_clip = txt_clip.fx(vfx.fadein, 0.2).fx(vfx.fadeout, 0.2)
    txt_clip = txt_clip.set_opacity(1)

    # BPMã«ã‚ˆã‚‹åŸºç¤ãƒªã‚ºãƒ å‘¨æ³¢æ•°
    bpm_freq = (bpm / 60) * 2 * np.pi  # â‰’ 12

    # ----------- ãƒ™ãƒ¼ã‚¹ (å¸¸æ™‚æºã‚Œ) -----------
    # å…¨ã¦ã®å­—å¹•ãŒBPMã«åˆã‚ã›ã¦ä¸Šä¸‹ã«å°ã•ãã‚†ã‚Œã‚‹
    txt_clip = txt_clip.set_position(lambda t: ('center', video_height * 0.8 + 5 * np.sin(bpm_freq * t)))

    # ----------- æ„Ÿæƒ…åˆ¥è¿½åŠ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ -----------
    emotion_effects = {
        "å–œ": [  # æ‹ã«åˆã‚ã›ã¦ã‚†ã£ãã‚Šã‚¹ã‚±ãƒ¼ãƒ«
            lambda clip: clip.fx(vfx.resize, lambda t: 1 + 0.03 * np.sin(bpm_freq * t))
        ],
        "æ€’": [  # æ‹ã«åˆã‚ã›ãŸå·¦å³å°åˆ»ã¿æŒ¯å‹•
            lambda clip: clip.set_position(lambda t: (
                video_width // 2 - clip.w // 2 + 5 * np.sin(bpm_freq * 2 * t),
                video_height * 0.8 - clip.h // 2 + 5 * np.sin(bpm_freq * t)
            ))
        ],
        "å“€": [  # ä¸‹ã’ã¤ã¤é¼“å‹•ã®ã‚ˆã†ã«å°‘ã—ã‚¹ã‚±ãƒ¼ãƒ«
            lambda clip: clip.set_position(lambda t: ('center', video_height * 0.82 + 3 * np.sin(bpm_freq * 0.5 * t))),
            lambda clip: clip.fx(vfx.resize, lambda t: 1 + 0.01 * np.sin(bpm_freq * 0.5 * t))
        ],
        "æ¥½": [  # BPMã«åˆã‚ã›ãŸè»½ã„å·¦å³ã‚¹ã‚¤ãƒ³ã‚°ï¼‹å°å›è»¢
            lambda clip: clip.set_position(lambda t: (video_width//2 + 10 * np.sin(bpm_freq * t), video_height * 0.8 + 5 * np.sin(bpm_freq * t))),
            lambda clip: clip.rotate(lambda t: 2 * np.sin(bpm_freq * t))
        ]
    }

    for effect in emotion_effects.get(emotion, []):
        txt_clip = effect(txt_clip)

    return txt_clip






def create_subtitle_clips(subtitles, offvocal_path, video_width=1920, video_height=1080):
    print("[INFO] ğŸ¬ æ„Ÿæƒ…ã«å¿œã˜ãŸå­—å¹•ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆä¸­...")
    subtitle_clips = []
    font_size = int(video_height * 0.06)
    
    #bpmè§£æ
    bpm = get_bpm(offvocal_path)
    
    #å­—å¹•æƒ…å ±ã¨ã‚Šã ã—
    for sub in subtitles:
        start_time = sub.start.total_seconds()
        end_time = sub.end.total_seconds()
        duration = end_time - start_time
        text = sub.content.replace('\n', ' ')

        emotion = get_dominant_emotion(text)

        # ---- â‘  å½± ----
        shadow_clip = TextClip(
            text,
            fontsize=font_size,
            font="/Library/Fonts/cinecaption226.ttf",
            color="gray",
            stroke_color=None,
            size=(video_width * 0.8, None),
            method="label"
        ).set_duration(duration).set_position((2, 2))  # ã“ã“ã ã‘ç›¸å¯¾çš„ã«ãšã‚‰ã™

        # ---- â‘¡ æœ¬ä½“ ----
        txt_clip = TextClip(
            text,
            fontsize=font_size,
            font="/Library/Fonts/cinecaption226.ttf",
            color="black",
            stroke_color="black",
            stroke_width=3,
            size=(video_width * 0.8, None),
            method="label"
        ).set_duration(duration)

        # ---- â‘¢ å½±ï¼‹æœ¬ä½“ã‚’åˆæˆ (ä½ç½®ã¯æ±ºã‚ãªã„)
        combo_clip = CompositeVideoClip([shadow_clip, txt_clip]).set_duration(duration)
        
        # â†“â†“ combo_clipã«ã ã‘apply_advanced_animationã‚’ã™ã‚‹
        combo_clip = apply_advanced_animation(combo_clip, video_width, video_height, bpm, emotion)
        combo_clip = combo_clip.set_start(start_time)

        subtitle_clips.append(combo_clip)

    return subtitle_clips






#æœ€çµ‚å‹•ç”»åˆæˆé–¢æ•°

def create_final_video(video, subtitles, composite_audio, output_path):
    print(f"[INFO] ğŸ¥ æœ€çµ‚å‹•ç”»ã‚’åˆæˆä¸­: {output_path}")
    video = video.resize((1920, 1080))
    video_duration = video.duration
    composite_audio = composite_audio.set_duration(video_duration)

    # ç”»åƒèª­ã¿è¾¼ã¿
    logo = (
    ImageClip("logo.png")
    .set_duration(175)
    .resize(height=700)
    .set_position(("center", "center"))
    .set_start(0)
    # .fadein(0.5).fadeout(0.5)
)

    #åˆæˆï¼ˆå‹•ç”»ï¼‹å­—å¹•ï¼‹ç”»åƒï¼‰
    elements = [video] + subtitles + [logo]

    final_clip = (CompositeVideoClip(elements, use_bgclip=True)
                  .set_duration(video_duration)
                  .set_audio(composite_audio))

    final_clip.write_videofile(output_path, fps=video.fps, codec="libx264", audio_codec="aac")
    

#å¤‰æ•°å®šç¾©
def get_bpm(filename):
    sampling_rate = 44100                                           #ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
    
    #æ¼”ç®—
    y, sr = librosa.load(path=filename, sr = sampling_rate)         #éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1æ¬¡å…ƒã®NumPyæµ®å‹•å°æ•°ç‚¹é…åˆ—ï¼ˆå¤‰æ•°yï¼‰ã«æ ¼ç´
    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)        #ãƒ†ãƒ³ãƒã‚’ç®—å‡º
    
    #çµæœè¡¨ç¤º
    #print('Estimated tempo: {:.2f} beats per minute'.format(tempo)) #çµæœã‚’è¡¨ç¤º
    print(tempo)

    return 115


# -----------------------------------------
# ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†
# -----------------------------------------

def main():
    vocal_path = "vocal.mp3"
    offvocal_path = "yoidore_offvocal.wav"
    temp_wav_path = "composite_audio.wav"
    video_path = "output_video_odiospe2.mp4"
    srt_path = "output_subtitles.srt"
    output_video = "final_output.mp4"
    offset_seconds = 8.695
    model_size = "medium"
    
    
    print("ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆæˆä¸­...")
    vocal_audio = AudioFileClip(vocal_path).volumex(1.2)
    offvocal_audio = AudioFileClip(offvocal_path).volumex(1.0)
    duration = min(vocal_audio.duration, offvocal_audio.duration)
    composite_audio = CompositeAudioClip([vocal_audio, offvocal_audio]).set_duration(duration)

    print(f"ğŸ¼ åˆæˆéŸ³å£°ã‚’ '{temp_wav_path}' ã«ä¿å­˜ä¸­...")
    composite_audio.write_audiofile(temp_wav_path, fps=44100, codec='pcm_s16le')
    '''
    print("ğŸ“œ å­—å¹•ã‚’ç”Ÿæˆä¸­...")
    generate_subtitles(vocal_path, srt_path, offset_seconds, model_size)
    '''
    print("ğŸ“– å­—å¹•ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    with open(srt_path, "r", encoding="utf-8") as f:
        subtitles = create_subtitle_clips(srt.parse(f.read()), offvocal_path, 1920, 1080)

    print("ğŸ¥ æœ€çµ‚å‹•ç”»ã‚’ç”Ÿæˆä¸­...")
    video = VideoFileClip(video_path).resize((1920, 1080)).set_duration(duration)
    create_final_video(video, subtitles, composite_audio, output_video)

    print("âœ… å®Œäº†ï¼å‡ºåŠ›:", output_video)

if __name__ == "__main__":
    main()
